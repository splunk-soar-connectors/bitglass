# File: app/bg.py
#
# Author: alexeiyur AT g m 4 i l . c 0 m
# Licensed under the MIT License (https://mit-license.org/)

import json
import os
import socketserver
import base64
import copy
import time
from threading import Thread, Condition
from datetime import datetime, timedelta
# from threading import get_ident

from urllib.error import HTTPError
import requests
from requests.auth import HTTPBasicAuth, AuthBase


import app

# This is the inconvenience to keep the static security scanners happy (could have used import * instead)
from app.consts import GC_LOGTYPE_CLOUDAUDIT, \
                       GC_LOGTYPE_ACCESS, \
                       GC_LOGTYPE_ADMIN, \
                       GC_LOGTYPE_CLOUDSUMMARY, \
                       GC_LOGTYPE_SWGWEB, \
                       GC_LOGTYPE_SWGWEBDLP, \
                       GC_LOGTYPE_HEALTHPROXY, \
                       GC_LOGTYPE_HEALTHAPI, \
                       GC_LOGTYPE_HEALTHSYSTEM, \
                       GC_FIELD_LOGTYPE, \
                       GC_FIELD_NEXTPAGETOKEN, \
                       GC_FIELD_INGESTEDTIME, \
                       GC_FIELD_INITIALTIME

from app.env import UpdateDataPath, datapath, loggingpath
from app.config import open_atomic, setPythonLoggingLevel, setPythonLogging, versionTuple
from app.configForward import ConfigForward, log_types
from app.logevent import pushLog


conf = None
lastLogFile = None


# For now, just using the token, never need to get/refresh one automatically (by using requests_oauthlib)
class OAuth2Token(AuthBase):
    def __init__(self, access_token):
        self.access_token = access_token

    def __call__(self, request):
        request.headers['Authorization'] = 'Bearer {0}'.format(
            self.access_token
        )
        return request


def ingestLogEvent(ctx, d, address, logTime):
    if ctx and ctx.ctx is not None:
        ctx.ctx.bgPushLogEvent(d, address, logTime)

    # TODO Prevent recursion sending to itself with syslog socket
    return pushLog(d, address, logTime)


def flushLogEvents(ctx):
    if ctx and ctx.ctx is not None:
        ctx.ctx.bgFlushLogEvents()


def Initialize(ctx, datapath=datapath, skipArgs=False, _logger=None, _conf=None, context=None):
    global conf
    global lastLogFile

    # Monkey patch env.datapath first with the value from the command line to read bg json configs
    updatepath = False
    if datapath:
        updatepath = UpdateDataPath(datapath)

    if not app.logger and not _logger:
        app.logger = setPythonLogging(None, datapath)

    if not datapath:
        # Put in the same directory as the logging file (the latter would be well-defined, without uuids)
        datapath = os.path.split(loggingpath)[0] + os.sep
        updatepath = UpdateDataPath(datapath)
        if updatepath:
            conf = None

    if not conf or updatepath:
        if not _conf:
            conf = ConfigForward(context)
            # Be sure to update the logging level once the config is loaded
            setPythonLoggingLevel(app.logger, conf)

            if not skipArgs or conf._isEnabled('debug'):
                # Parse and apply command line options. Always process for a local dev run ('debug'), it's compatible
                conf._applyOptionsOnce()
        else:
            conf = _conf

    # Override the configuration
    if ctx and ctx.ctx is not None:
        # Override the config settings and disable daemon mode for explicit cli context
        ctx.ctx.bgLoadConfig(conf)
        conf._isDaemon = False  # type: ignore[union-attr]
    conf._calculateOptions()    # type: ignore[union-attr]

    if not lastLogFile or updatepath:
        cnf = _conf or conf

        # For Splunk app upgrade, manually 'cp lastlog.json ../local/' before upgrading to ingest incrementally
        # This is because it was saved in default/ in the previous version 1.0.8 and default/ is yanked during upgrade
        folder = cnf._folder    # type: ignore[union-attr]
        if (os.path.sep + 'default') in cnf._folder:    # type: ignore[union-attr]
            folder = os.path.join(folder, '..', 'local', '')

        lastLogFile = LastLog(os.path.join(folder, 'lastlog'), context)

    return conf


class SyslogUDPHandler(socketserver.BaseRequestHandler):
    kwargs = None
    callback = None

    @classmethod
    def start(cls, callback, host, port=514, poll_interval=0.5, **kwargs):
        cls.kwargs = kwargs
        cls.callback = callback
        # TODO Test exception propagation to the main thread (due to a bad host?), may need handling
        try:
            server = socketserver.UDPServer((host, port), cls)
            server.serve_forever(poll_interval=poll_interval)
        except (IOError, SystemExit):
            raise
        except KeyboardInterrupt:
            app.logger.info("Crtl+C Pressed. Shutting down.")

    def handle(self):
        # logger = self.kwargs['logger']
        data = bytes.decode(self.request[0].strip())

        # Strip the string and convert to json
        try:
            conf = self.kwargs['conf']      # type: ignore[index]
            condition = self.kwargs['condition']    # type: ignore[index]

            s = '%s :{' % conf.customer.lower()
            start = data.find(s) + len(s) - 1
            end = data.rfind('}') + 1
            logData = json.loads('{"response":{"data":[' + data[start:end] + ']}}')

            with conf._lock(condition, notify=False):
                transferLogs(None, logTypes=None, logData=logData, **self.kwargs)   # type: ignore[misc]  # pylint: disable=E1134

        except Exception as ex:
            app.logger.warning('{0}\n - Discarded bad event message in syslog stream:\n"{1}"\n- from sender {2}'.format(
                str(ex),
                data,
                self.client_address[0])
            )
            return


TIME_FORMAT_URL = '%Y-%m-%dT%H:%M:%SZ'
TIME_FORMAT_LOG = '%d %b %Y %H:%M:%S'
TIME_FORMAT_ISO = '%Y-%m-%dT%H:%M:%S.%fZ'


def strptime(s):
    # For more reliable (but slower) datetime parsing (non-English locales etc.) use:
    # pip install python-dateutil
    # from dateutil import parser
    # parser.parse("Aug 28 1999 12:00AM")  # datetime.datetime(1999, 8, 28, 0, 0)
    # '06 Nov 2018 08:15:10'

    return datetime.strptime(s, TIME_FORMAT_LOG)


class LastLog:
    def __init__(self, fname, context=None, shared=None, logtype=''):
        self.fname = '{0}-{1}.json'.format(fname, logtype) if shared else '{0}.json'.format(fname)
        self.shared = shared
        self.log = {}
        self.logtype = logtype
        self.subLogs = {}
        self.context = context
        try:
            if self.context is not None:
                # No file system, also may be flat (the old format). That's Demisto limitation so far
                self.log = (({logtype: json.loads(context[logtype])}
                             if isinstance(context[logtype], str)
                             else {logtype: context[logtype]})
                            if logtype in context
                            else {logtype: {}}) if self.shared else dict(context)
            else:
                with open(self.fname, 'r') as f:
                    self.log = json.load(f)

            if self.shared is None:
                # This is a shared (old) file (or string-only context). Convert to the new format if needed
                for lt in log_types:
                    if self.get(logtype=lt) and isinstance(self.log[lt], str):
                        self.log[lt] = json.loads(self.log[lt])
        except Exception as ex:
            app.logger.info('{0}\n - Last log file {1} not found'.format(str(ex), self.fname))
            lastLog = {}
            if self.shared:
                lastLog[logtype] = self.shared.log[logtype]
            else:
                for lt in log_types:
                    lastLog[lt] = {}
            self.log = lastLog

        # Create children one per log type unless sharing the same file for all
        if self.shared is None:
            for lt in log_types:
                self.subLogs[lt] = LastLog(fname, context, self, lt)

    def dump(self):
        try:
            if self.context is not None:
                # Don't assume it has to be converted to string, maybe some platform support nested context (unlike Demisto)
                # The save() method below will take care of that and merging different log types
                self.context[self.logtype] = self.log[self.logtype]
                self.context.save()
            else:
                with open_atomic(self.fname, 'w') as f:
                    json.dump(self.log, f, indent=4, sort_keys=True)
        except Exception as ex:
            app.logger.error('Could not save last log event across app sessions: %s' % ex)

    def get(self, field=None, logtype=None):
        if logtype is None:
            logtype = self.logtype
        elif logtype in self.subLogs:
            return self.subLogs[logtype].get(field)

        if logtype not in self.log or self.log[logtype] == {}:
            return False

        if field:
            ll = self.log[logtype]
            # Handle the old format to be forward compatible across upgrade
            res = json.loads(ll) if isinstance(ll, str) else ll
            return res[field] if field in res else None

        return True

    def getHistoricLogTypeList(self):
        return [lt for lt in log_types if self.get(logtype=lt)]

    def update(self, dtime, ll, logtype=None):
        if logtype is None:
            logtype = self.logtype
        elif logtype in self.subLogs:
            return self.subLogs[logtype].update(dtime, ll)

        if ll:
            # Add extra fields for diagnostics. Should not lag event log timestamp more than by the API polling interval
            ll[GC_FIELD_INGESTEDTIME] = datetime.utcnow().strftime(TIME_FORMAT_LOG)

            # For diagnostics purposes (for some weird cases of limited SIEM license issues or deployment irregularities etc.),
            # keep the earliest (initial) timestamp of past requests starting from the most recent reset (log rewind)
            # This way we can prove that the missing data is due to some SIEM platform vagary rather than our bug
            it = self.get('_initialtime', logtype)
            if it:
                # Copy over, this never changes unless logs get rewound
                ll[GC_FIELD_INITIALTIME] = it
            else:
                # This is the first successful save
                ll[GC_FIELD_INITIALTIME] = dtime.strftime(TIME_FORMAT_LOG)

            self.log[logtype] = ll
            self.dump()
        else:
            # This is a successful request with empty (exhausted) data so use the last one but handle the (corner) case
            # of error logged inbetween (coinsiding with app relaunch) by clearing the error entries if there are any
            if self.get(logtype=logtype):
                if self.get('_failedtime', logtype):
                    del self.log[logtype]['_failedtime']
                if self.get('_errormessage', logtype):
                    del self.log[logtype]['_errormessage']

        if logtype in self.log:
            return json.dumps(self.log[logtype])
        else:
            return ''

    def updateError(self, conf, errormsg, resettime, logtype=None):
        if logtype is None:
            logtype = self.logtype
        elif logtype in self.subLogs:
            return self.subLogs[logtype].updateError(conf, errormsg, resettime)

        if not self.get(logtype=logtype):
            self.log[logtype] = {}

        # Handle the corner case of never having a successful message ever yet so add the missing time
        # field to make the recovery possible (the data reading code defaults to 'now' in such case to
        # play it safe and avoid the possible data duplication but this leads to never getting
        # good messages unless by fluke of hitting upon a very recent message). Assume the initial
        # data period just the same as the reading code does when starting up.
        # This provides for an alternative hack to reset the log type: edit the file and rename the 'time' field.
        if not self.get('time', logtype) or resettime:
            if self.get('time', logtype):
                # Backup the original 'time' field if present by renaming it first
                self.log[logtype]['_time'] = self.log[logtype]['time']
            t = datetime.utcnow() + timedelta(seconds=-1 * conf.log_initial) if not resettime else resettime
            self.log[logtype]['time'] = t.strftime(TIME_FORMAT_LOG)

        # Update with failure timestamp and message, keep last ingested success timestamp
        self.log[logtype]['_failedtime'] = datetime.utcnow().strftime(TIME_FORMAT_LOG)
        self.log[logtype]['_errormessage'] = str(errormsg)
        self.dump()

    def clobber(self, resettime=None, logtype=None):
        # Clobbering the children with empty json {} is the simplest and most robust way to go.
        # Actually deleting the file would not be enough as the old format file persist, so
        # implementing it correctly would need atomically updating more than one file which would
        # complicate the code immensely and for Splunk, even the use of temp files is a potential
        # certification problem (although bogus one IMO)
        if logtype is None:
            logtype = self.logtype
        elif logtype in self.subLogs:
            return self.subLogs[logtype].clobber(resettime)

        # Just write bare braces (without adding the log type) keeping it simpler
        # self.log[logtype] = {}
        self.log = {}

        # The softer option of rolling back the time (provided by the UI already for flexibility)
        # TODO Ignore for the simplicity sake until the UI provides the actual reset time
        # if resettime:
        #     self.log[logtype] = {}
        #     self.log[logtype]['time'] = resettime.strftime(TIME_FORMAT_LOG)

        self.dump()


def isOldLogType(lt):
    """ Different data order (specific to the storage on the server), npt format etc.
    """
    return lt in [GC_LOGTYPE_ACCESS, GC_LOGTYPE_ADMIN, GC_LOGTYPE_CLOUDAUDIT, GC_LOGTYPE_CLOUDSUMMARY,
                  # Old APIs do return health* data (a bug) and npt is in the old format anyways!
                  GC_LOGTYPE_HEALTHPROXY, GC_LOGTYPE_HEALTHAPI, GC_LOGTYPE_HEALTHSYSTEM]


def getAPIToken(logData, conf, logType):
    if not conf.useNextPageToken:
        return None

    try:
        token = logData['nextpagetoken']
        d = json.loads(base64.b64decode(token))
    except Exception as ex:
        app.logger.warning('Invalid token returned: %s %s' % (token, ex))   # nosemgrep
        return None

    if d == {}:
        # Technically, it's an invalid token but this accompanies an empty data set
        return None

    if isOldLogType(logType):
        if 'log_id' not in d:
            app.logger.warning('No "log_id" encoded in logtype "%s" returned token: %s' % (logType, token))     # nosemgrep
            return None

        if 'datetime' not in d:
            app.logger.warning('No "datetime" encoded in logtype "%s" returned token: %s' % (logType, token))   # nosemgrep
            return None
    else:
        if 'start_time' not in d:
            app.logger.warning('No "start_time" encoded in logtype "%s" returned token: %s' % (logType, token)) # nosemgrep
            return None

        if 'end_time' not in d:
            app.logger.warning('No "end_time" encoded in logtype "%s" returned token: %s' % (logType, token))   # nosemgrep
            return None

        if 'page' not in d:
            app.logger.warning('No "page" encoded in logtype "%s" returned token: %s' % (logType, token))       # nosemgrep
            return None

    return token


SKIPPED_REQUEST_ERROR = 'UNAUTHORiZED'


def RestParamsLogs(_, host, api_ver, logType, npt, dtime):
    url = ('https://portal.' + host) if host else ''
    endpoint = '/api/bitglassapi/logs'

    # Adjust the version upwards to the minimum supported version for new log types as necessary
    # Make sure it's lower before overriding to avoid downgrading the version
    if logType in [GC_LOGTYPE_SWGWEB, GC_LOGTYPE_SWGWEBDLP]:
        if versionTuple(api_ver) < versionTuple('1.1.0'):
            api_ver = '1.1.0'
    elif logType in [GC_LOGTYPE_HEALTHPROXY, GC_LOGTYPE_HEALTHAPI, GC_LOGTYPE_HEALTHSYSTEM]:
        if versionTuple(api_ver) < versionTuple('1.1.5'):
            api_ver = '1.1.5'

    if npt is None:
        urlTime = dtime.strftime(TIME_FORMAT_URL)
        dataParams = '/?cv={0}&responseformat=json&type={1}&startdate={2}'.format(api_ver, str(logType), urlTime)
    else:
        dataParams = '/?cv={0}&responseformat=json&type={1}&nextpagetoken={2}'.format(api_ver, str(logType), npt)

    return (url, endpoint, dataParams)


def RestParamsConfig(_, host, api_ver, type_, action):
    url = ('https://portal.' + host) if host else ''

    # This is a POST, version is not a proper param, unlike in logs (?? for some reason)
    endpoint = '/api/bitglassapi/config/v{0}/?type={1}&action={2}'.format(api_ver, type_, action)
    return (url, endpoint)


def restCall(_,
             url, endpoint, dataParams,
             auth_token,
             proxies=None,
             verify=True,
             username=None,
             password=None):
    if dataParams is None:
        dataParams = ''

    # BANDIT    No, this is not a 'hardcoded password'. This is a check if any password supplied by the user.
    if auth_token is None or auth_token == '':  # nosec
        auth_type = 'Basic'

        # Must have creds supplied for basic
        if any(a() for a in [lambda: username is None or username == '',
                             lambda: password is None or password == '']):    # nosec
            # BANDIT    ^^^ No, this is not a 'hardcoded password'. This is a check if any password supplied by the user.
            # Emulate an http error instead of calling with empty password (when the form initially loads)
            # to avoid counting against API count quota
            raise HTTPError(url + endpoint, 401, SKIPPED_REQUEST_ERROR, {}, None)

        auth_token = base64.b64encode((username + ':' + password).encode('utf-8'))
        auth_token = auth_token.decode('utf-8')
    else:
        auth_type = 'Bearer'

    # Use requests by default if available
    r = None

    # The authentication header is added below
    headers = {'Content-Type': 'application/json'}

    d = {}
    with requests.Session() as s:
        if auth_type == 'Basic':
            s.auth = HTTPBasicAuth(username, password)
        else:
            s.auth = OAuth2Token(auth_token)

        if proxies is not None and len(proxies) > 0:
            if isinstance(proxies, dict):
                s.proxies = proxies
            elif proxies[1] is None:
                # Assume a tuple, no passwords provided (or merged in already) - nothing to merge
                s.proxies = proxies[0]
            else:
                # Assume a tuple and merge in the passwords
                s.proxies = ConfigForward._mergeProxiesPswd(proxies[0], proxies[1])

        if isinstance(dataParams, dict):
            # Assume json
            r = s.post(url + endpoint, headers=headers, verify=verify, json=dataParams)
        else:
            # TODO Inject failures (including the initial failure) for testing: raise Exception('test')
            r = s.get(url + endpoint + dataParams, headers=headers, verify=verify)

        r.raise_for_status()
        d = r.json()

    return d, r


def RestCall(_, endpoint, dataParams):
    return restCall(
        _,
        'https://portal.' + conf.host,  # type: ignore[union-attr]
        endpoint,
        dataParams,
        conf._auth_token.pswd,  # type: ignore[union-attr]
        (conf.proxies, conf._proxies_pswd.pswd),    # type: ignore[union-attr]
        conf.verify,    # type: ignore[union-attr]
        conf._username,     # type: ignore[union-attr]
        conf._password.pswd     # type: ignore[union-attr]
    )


def RestCallConfig(_, endpoint, dataParams):
    """ Do basic exception handling, just what is necessary for the Bitglass API for integration into SIEM
    """
    try:
        return RestCall(_, endpoint, dataParams)
    except Exception as ex:
        msg = 'Bitglass Config API: request %s(%s) failed with %s' % (endpoint, dataParams, ex)
        # Avoid polluting the log if no valid settings have been set yet
        if SKIPPED_REQUEST_ERROR not in msg:
            app.logger.error(msg)

        return {}, ex


def processLogEvents(ctx, conf, logType, data, nextPageToken, logTime, isSyslog):
    # Cover the case of reverse chronological order (in case of not reversing it back)
    lastLog = data[0]

    # Querying API data by 'time' field (not using nextpagetoken) is broken for 1.1.0 log types
    # swgweb and swgwebdlp causing overlaps. So disable the fallback path for them (no nextpagetoken)
    # No fix planned, so this workaround is a keeper
    isOldLt = isOldLogType(logType)
    if nextPageToken is None and not isOldLt and not isSyslog:
        raise ValueError('Invalid page token for swgweb* log types is not supported')

    for d in data[::-1 if strptime(data[0]['time']) > strptime(data[-1]['time']) else 1]:
        # In some new log types like swgweb the data are sorted from recent to older
        # So let's not assume chronological order to be on the safe side..
        tm = strptime(d['time'])

        # Inject logtype field, it's needed by QRadar Event ID definition (defined in DSM editor)
        if GC_FIELD_LOGTYPE not in d:
            d[GC_FIELD_LOGTYPE] = logType

        # NOTE Use logTime if QRadar has problems with decreasing time (as in swgweb and swgwebdlp)
        ingestLogEvent(ctx, d, conf._syslogDest, tm)

        d[GC_FIELD_NEXTPAGETOKEN] = nextPageToken if nextPageToken is not None else ''

        if any(a() for a in [lambda: logTime is None,
                             lambda: tm > logTime,
                             lambda: isOldLt]):
            # ^^^ This is to avoid the possible +1 sec skipping data problem (if no npt)
            logTime = tm
            lastLog = d
            # json.dumps(d, sort_keys=False, indent=4, separators = (',', ': '))

    return logTime, lastLog


def rewindLogEvents(ctx, conf, logType):
    # Must be validated by the app if it's the actual time datetime.datetime
    # TODO Can switch to using format TIME_FORMAT_ISO for conf._reset_time here and in the UI
    dtime = datetime.utcnow() + timedelta(seconds=-1 * conf.log_initial)
    nextPageToken = None

    # Override the last log data with the new 'time' field
    if conf.hardreset:
        # This should be the default.
        # Clobber all the data in the file as the last resort! This is the important fool-proof method for
        # ultimate UI control on the cloud if some bug is suspected to hold new messages etc.
        app.logger.warning('Hard-reset for "%s" initiated due to user request. '
                           'The data will resume when new messages get available.' % logType)
        lastLogFile.clobber(dtime, logtype=logType)     # type: ignore[union-attr]
    else:
        # The soft reset mode is essential for testing by keeping the state around. It's used for auto-reset as well
        app.logger.warning('Soft-reset for "%s" initiated due to user request. '
                           'The data will resume when new messages get available.' % logType)
        lastLogFile.updateError(conf, 'Soft-reset initiated due to user request. '  # type: ignore[union-attr]
                                'Waiting for the new data becoming available starting from (see the \'time\' field below)',
                                dtime, logType)

    return nextPageToken, dtime


def drainLogEvents(ctx, conf, logType, logData=None, dtime=None, nextPageToken=None):

    status = conf.status[logType]
    r = None

    isSyslog = (logData is not None)

    if conf._reset_time and not isSyslog:
        nextPageToken, dtime = rewindLogEvents(ctx, conf, logType)

    logTime = dtime

    try:
        raiseBackendError = False
        if raiseBackendError:
            raise Exception('injectbackendfailure')

        i = 0
        drained = False
        while not drained:
            if isSyslog:
                drained = True
            else:
                if i > 0:
                    # This is a crude way to control max event rate for Splunk / QRadar etc. as required
                    # without adding another thread and a queue which is a design over-kill
                    time.sleep(1.0 / conf._max_request_rate)

                if conf.host == conf._default.host:
                    # Avoid the overhead of invalid request even if there is no traffic generated
                    raise HTTPError(conf.host, -2, SKIPPED_REQUEST_ERROR, {}, None)

                # TODO If there is a hint from API that all data is drained can save the
                # split second sleep and the extra request
                url, endpoint, dataParams = RestParamsLogs(None,
                                                           conf.host,
                                                           conf.api_ver,
                                                           logType,
                                                           nextPageToken,
                                                           logTime + timedelta(seconds=1))
                logData, r = restCall(None,
                                      url, endpoint, dataParams,
                                      conf._auth_token.pswd,
                                      (conf.proxies, conf._proxies_pswd.pswd),
                                      conf.verify,
                                      conf._username,
                                      conf._password.pswd)
                i += 1

            lastLog = None
            nextPageToken = getAPIToken(logData, conf, logType)

            data = logData['response']['data']
            if len(data) == 0:
                drained = True
            else:
                logTime, lastLog = processLogEvents(ctx, conf, logType, data, nextPageToken, logTime, isSyslog)

            status.cntSuccess = status.cntSuccess + 1
            status.lastMsg = 'ok'
            if isSyslog:
                status.lastLog = json.dumps(lastLog)
            else:
                status.lastLog = lastLogFile.update(dtime, lastLog, logType)    # type: ignore[union-attr]

            flushLogEvents(ctx)

    except Exception as ex:
        msg = 'Bitglass Log Event Polling: failed to fetch data "%s": %s' % (str(logType), ex)
        # If no valid settings have been set, avoid polluting the log
        if SKIPPED_REQUEST_ERROR not in msg:
            app.logger.error(msg)
            r = ex
            lastLogFile.updateError(conf, r, None, logType)     # type: ignore[union-attr]
            status.cntError = status.cntError + 1
            status.lastMsg = str(ex)

        status.lastLog = ''

    # NOTE  Last successful result has empty data now (drained), instead, could merge all data and return
    #       making it optional if ingestLogEvent is not set.. Without it, attaching data to result is rather useless
    status.lastRes = r
    status.lastTime = logTime
    status.updateCount = conf.updateCount

    conf.status['last'] = status

    return logTime


def transferLogs(ctx, conf, condition, logTypes=None, logData=None, dtime=None,
                 npt=None, resetFence=datetime.utcnow().isoformat()):
    myConf = conf._deepcopy()
    condition.release()

    if not logTypes:
        if myConf._reset_time:
            # Pull all the log types that have ever been pulled unless the specific log type list was provided (like in Phantom).
            # Merge with the currently specified ones
            # logTypes = log_types   # All possibly supported log types
            h = lastLogFile.getHistoricLogTypeList()    # type: ignore[union-attr]
            n = myConf.log_types
            logTypes = h + list(set(n) - set(h))   # Whatever have been tried from the last reset plus currently requested
        else:
            logTypes = myConf.log_types    # Only currently requested

    logTime = {}
    if logData is None:
        for lt in logTypes:
            logTime[lt] = drainLogEvents(ctx, myConf, lt, logData,
                                         dtime[lt] if dtime is not None else None,
                                         npt[lt] if npt is not None else None)
    else:
        # syslog source
        # Make sure nextpagetoken is disabled
        myConf.useNextPageToken = False
        lt = logData['response']['data'][0]['logtype']
        logTime[lt] = drainLogEvents(None, myConf, lt, logData=logData)

    condition.acquire()
    myConf.status['updateCount'] = conf.updateCount

    # Load the latest state for the UI and reset the read-once-reset-to-default config params
    conf.status = copy.deepcopy(myConf.status)
    if conf._reset_time:
        if conf._isForeignConfigStore():
            conf.reset_fence = resetFence
            conf._save()
        conf._reset_time = ''

    # Increment by smallest delta to avoid repeating same entries
    # TODO Using microseconds=1 causes event duplication.. what is the minimum resolution to increment??
    #       without data loss but with guaranteed no repetitions
    if logData is None:
        if conf._isDaemon:
            condition.wait(myConf.log_interval)
        for lt in logTypes:
            dtime[lt] = logTime[lt] + timedelta(seconds=1)


def getLastLogTimeAndNpt(ctx, conf, logTypes=None):
    # Have to complicate things b/c the API doesn't support combining different log types
    dtime = {}
    npt = {}    # type: ignore[var-annotated]
    now = datetime.utcnow()
    for lt in log_types:
        # = datetime.utcnow() + timedelta(days=-1)
        dtime[lt] = now + timedelta(seconds=-1 * conf.log_initial)
        npt[lt] = None

        # Adjust to avoid the overlap with a previous run, warn on a possible gap
        # The gap is caused by either: app down time or the log source being disabled in earlier app run
        # was greater than 30 days (default of 'log_initial')
        try:
            if lastLogFile.get(logtype=lt):     # type: ignore[union-attr]
                try:
                    # Could be missing due to the old file format
                    npt[lt] = lastLogFile.get('nextpagetoken', lt)  # type: ignore[union-attr]
                    if npt[lt] == '':
                        npt[lt] = None
                        app.logger.warning('Empty nextpagetoken found for log type %s (old lastlog format?)' % lt)

                except Exception as ex:
                    npt[lt] = None
                    app.logger.warning('Error "%s" retrieving nextpagetoken for log type %s' % (str(ex), lt))

                d = strptime(lastLogFile.get('time', lt))   # type: ignore[union-attr]
                if dtime[lt] <= d:
                    dtime[lt] = d
                else:
                    # Important! For a possible gap, discard nextpagetoken loaded from lastlog
                    # NOTE: This still has an extremely remote possibility of data duplication
                    #       (no messages over the gap period is a necessary condition then - unpopulated gap)
                    npt[lt] = None
                    app.logger.warning('Possible gap for log type %s from %s to %s' %
                                       (str(lt),
                                           d.strftime(TIME_FORMAT_LOG),
                                           dtime[lt].strftime(TIME_FORMAT_LOG))
                                       )
            else:
                app.logger.info('No lastlog data found for log type %s' % lt)

        except Exception as ex:
            # Bad data in lastLogFile? Treat overlap as data corruption so exclude its possibility and warn
            # Discard nextpagetoken loaded from lastlog, also see the comment just above
            npt[lt] = None

            # By just setting it to 'now' the bad data would persist without getting any new messages and
            # hence no chance to reset the data to good format unless due to a fluke of a very recent message.
            # Instead, do a soft reset
            dtime[lt] = now
            app.logger.error('Probable gap for log type %s to %s due to BAD LAST LOG DATA: %s' %
                             (str(lt),
                                 dtime[lt].strftime(TIME_FORMAT_LOG),
                                 ex)
                             )

            # Re-write the file back to the good format
            # TODO If wishing to reduce missing a lot of data in favor of some overlap,
            #      may rely on the last log file mofification time minus polling period (which one? could have changed)
            app.logger.warning('Auto-reset initiated due to bad last log data. '
                               'The data will resume when new messages get available.')
            lastLogFile.updateError(conf, 'Auto-reset initiated due to bad last log data. '     # type: ignore[union-attr]
                                    'Waiting for the new data becoming available starting from (see the \'time\' field below)',
                                    now, lt)

            # Should never end up here again unless the file gets invalidated outside the app once more

        if npt[lt] is not None:
            app.logger.info('Valid_nextpagetoken "%s" found for log type %s' % (npt[lt], lt))

    return dtime, npt


def PollLogs(ctx, conf, logTypes=None, condition=Condition(), resetFence=datetime.utcnow().isoformat()):
    """
    Pump BG log events from BG API to QRadar
    """

    pid = os.getpid()
    # tid = get_ident()
    tid = 0
    app.logger.info('================================================================')
    app.logger.info('Polling: start polling log events.. pid=%s, tid=%s' % (pid, tid))
    app.logger.info('----------------------------------------------------------------')

    isSyslog = all(a() for a in [lambda: '://' not in conf.api_url,
                                 lambda: len(conf.api_url.split(':')) == 2])

    res = None
    if isSyslog:
        host, port = conf.api_url.split(':')
        # TODO: At least verify that sink_url is different to reduce the loop possibility sending back to itself
        SyslogUDPHandler.start(transferLogs,
                               host=host,
                               port=int(port),
                               conf=conf,
                               condition=condition
                               )
    else:
        dtime, npt = getLastLogTimeAndNpt(ctx, conf, logTypes)

        with conf._lock(condition, notify=False):
            isDaemon = True
            while isDaemon:
                transferLogs(ctx, conf, condition, logTypes, None, dtime, npt, resetFence)

                # Run only once if not in the daemon mode
                isDaemon = conf._isDaemon

        res = conf.status

    app.logger.info('Polling: stop polling log events.. pid=%s, tid=%s' % (pid, tid))
    app.logger.info('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
    return res


class bitglassapi:

    Initialize = Initialize

    # TODO Implement OverrideConfig(), it will also validate all settings (the validation is to be moved from UI)

    # Low level (overriding settings params)
    restCall = restCall

    RestParamsLogs = RestParamsLogs
    RestParamsConfig = RestParamsConfig

    RestCall = RestCall
    RestCallConfig = RestCallConfig

    # Higher level calls relying on serialized data and synchronization
    PollLogs = PollLogs

    def __init__(self, ctx=None):
        if ctx is None:
            # Use default callbacks
            ctx = self

        self.ctx = ctx

    # Default callbacks command mode without explicit context (like Splunk)
    def bgPushLogEvent(self, d, address, logTime):
        # Additional processing for the script
        from app import cli     # pylint: disable=E0401
        cli.pushLog(d, address, logTime)

    def bgFlushLogEvents(self):
        from app import cli     # pylint: disable=E0401
        cli.flushLogs()

    def bgLoadConfig(self, conf):
        from app import cli     # pylint: disable=E0401
        cli.loadConfiguration(conf)


# Test flat dict context (when file i/o is not available)
# This class gets overridden in the actual platform using such context
class BitglassContext(dict):
    def __init__(self, context={}):
        # Purge the old data
        for k, v in dict(self).items():
            del self[k]
        # Load the new data
        for k, v in context.items():
            self[k] = v

    def save(self):
        # Must convert into flat dict of strings or it's not saved properly
        d = {k: v if isinstance(v, str) else json.dumps(v) for k, v in dict(self).items()}

        # This (test code) runs repeatedly in a thread instead of passing from real framework on repeated script invocation
        # The above means the Initialize() path called only once with empty dict (making it a partial scenario), unlike in Demisto
        self.__init__(d)    # type: ignore[misc]


context = None


# Uncomment for testing flat file-less context like the one used in Demisto
# This is passed to bgapi to keep the lastlog object in since there is no file i/o available
# context = BitglassContext()


def startWorkerThread(conf, isDaemon=True, bgapi=None):

    Initialize(bgapi, _logger=app.logger, _conf=conf, context=context)

    condition = Condition()
    thread = Thread(target=PollLogs, args=(bgapi, conf, None, condition))

    conf._isDaemon = isDaemon
    thread.start()
    if not isDaemon:
        thread.join()
    return condition


# Skip this part for the merged module support
if __name__ == '__main__':

    from app.cli import main

    Initialize(None)

    # Only for debugging full context cli variants so that can use one debug setting for all
    if conf._isEnabled('debug'):
        main(app.logger, conf, bitglassapi)

    # Start the worker thread explicitly if main() above didn't exit()
    startWorkerThread(conf, False, bitglassapi())
